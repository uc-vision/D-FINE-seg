import math

import torch
import torch.nn as nn
import torch.nn.functional as F
from loguru import logger
from torch import Tensor
from torchvision.ops.boxes import box_area


def box_iou(boxes1: Tensor, boxes2: Tensor):
  area1 = box_area(boxes1)
  area2 = box_area(boxes2)

  lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]
  rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]

  wh = (rb - lt).clamp(min=0)  # [N,M,2]
  inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]

  union = area1[:, None] + area2 - inter

  iou = inter / union
  return iou, union


def generalized_box_iou(boxes1, boxes2):
  """
  Generalized IoU from https://giou.stanford.edu/

  The boxes should be in [x0, y0, x1, y1] format

  Returns a [N, M] pairwise matrix, where N = len(boxes1)
  and M = len(boxes2)
  """
  # degenerate boxes gives inf / nan results
  # so do an early check
  if not (boxes1[:, 2:] >= boxes1[:, :2]).all():
    logger.error(boxes1)
  assert (boxes1[:, 2:] >= boxes1[:, :2]).all()
  assert (boxes2[:, 2:] >= boxes2[:, :2]).all()
  iou, union = box_iou(boxes1, boxes2)

  lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])
  rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])

  wh = (rb - lt).clamp(min=0)  # [N,M,2]
  area = wh[:, :, 0] * wh[:, :, 1]

  return iou - (area - union) / area


def inverse_sigmoid(x: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:
  x = x.clip(min=0.0, max=1.0)
  return torch.log(x.clip(min=eps) / (1 - x).clip(min=eps))


def box_cxcywh_to_xyxy(x):
  x_c, y_c, w, h = x.unbind(-1)
  b = [
    (x_c - 0.5 * w.clamp(min=0.0)),
    (y_c - 0.5 * h.clamp(min=0.0)),
    (x_c + 0.5 * w.clamp(min=0.0)),
    (y_c + 0.5 * h.clamp(min=0.0)),
  ]
  return torch.stack(b, dim=-1)


def box_xyxy_to_cxcywh(x: Tensor) -> Tensor:
  x0, y0, x1, y1 = x.unbind(-1)
  b = [(x0 + x1) / 2, (y0 + y1) / 2, (x1 - x0), (y1 - y0)]
  return torch.stack(b, dim=-1)


def bias_init_with_prob(prior_prob=0.01):
  """initialize conv/fc bias value according to a given probability value."""
  bias_init = float(-math.log((1 - prior_prob) / prior_prob))
  return bias_init


def get_activation(act: str, inpace: bool = True):
  """get activation"""
  if act is None:
    return nn.Identity()

  elif isinstance(act, nn.Module):
    return act

  act = act.lower()

  if act == "silu" or act == "swish":
    m = nn.SiLU()

  elif act == "relu":
    m = nn.ReLU()

  elif act == "leaky_relu":
    m = nn.LeakyReLU()

  elif act == "silu":
    m = nn.SiLU()

  elif act == "gelu":
    m = nn.GELU()

  elif act == "hardsigmoid":
    m = nn.Hardsigmoid()

  else:
    raise RuntimeError("")

  if hasattr(m, "inplace"):
    m.inplace = inpace

  return m


def distance2bbox(points, distance, reg_scale):
  """
  Decodes edge-distances into bounding box coordinates.

  Args:
      points (Tensor): (B, N, 4) or (N, 4) format, representing [x, y, w, h],
                       where (x, y) is the center and (w, h) are width and height.
      distance (Tensor): (B, N, 4) or (N, 4), representing distances from the
                         point to the left, top, right, and bottom boundaries.

      reg_scale (float): Controls the curvature of the Weighting Function.

  Returns:
      Tensor: Bounding boxes in (N, 4) or (B, N, 4) format [cx, cy, w, h].
  """
  reg_scale = abs(reg_scale)
  x1 = points[..., 0] - (0.5 * reg_scale + distance[..., 0]) * (points[..., 2] / reg_scale)
  y1 = points[..., 1] - (0.5 * reg_scale + distance[..., 1]) * (points[..., 3] / reg_scale)
  x2 = points[..., 0] + (0.5 * reg_scale + distance[..., 2]) * (points[..., 2] / reg_scale)
  y2 = points[..., 1] + (0.5 * reg_scale + distance[..., 3]) * (points[..., 3] / reg_scale)

  bboxes = torch.stack([x1, y1, x2, y2], -1)

  return box_xyxy_to_cxcywh(bboxes)


def weighting_function(reg_max, up, reg_scale, deploy=False):
  """
  Generates the non-uniform Weighting Function W(n) for bounding box regression.

  Args:
      reg_max (int): Max number of the discrete bins.
      up (Tensor): Controls upper bounds of the sequence,
                   where maximum offset is ±up * H / W.
      reg_scale (float): Controls the curvature of the Weighting Function.
                         Larger values result in flatter weights near the central axis W(reg_max/2)=0
                         and steeper weights at both ends.
      deploy (bool): If True, uses deployment mode settings.

  Returns:
      Tensor: Sequence of Weighting Function.
  """
  if deploy:
    upper_bound1 = (abs(up[0]) * abs(reg_scale)).item()
    upper_bound2 = (abs(up[0]) * abs(reg_scale) * 2).item()
    step = (upper_bound1 + 1) ** (2 / (reg_max - 2))
    left_values = [-((step) ** i) + 1 for i in range(reg_max // 2 - 1, 0, -1)]
    right_values = [(step) ** i - 1 for i in range(1, reg_max // 2)]
    values = (
      [-upper_bound2]
      + left_values
      + [torch.zeros_like(up[0][None])]
      + right_values
      + [upper_bound2]
    )
    return torch.tensor(values, dtype=up.dtype, device=up.device)
  else:
    upper_bound1 = abs(up[0]) * abs(reg_scale)
    upper_bound2 = abs(up[0]) * abs(reg_scale) * 2
    step = (upper_bound1 + 1) ** (2 / (reg_max - 2))
    left_values = [-((step) ** i) + 1 for i in range(reg_max // 2 - 1, 0, -1)]
    right_values = [(step) ** i - 1 for i in range(1, reg_max // 2)]
    values = (
      [-upper_bound2]
      + left_values
      + [torch.zeros_like(up[0][None])]
      + right_values
      + [upper_bound2]
    )
    return torch.cat(values, 0)


def deformable_attention_core_func_v2(
  value: torch.Tensor,
  value_spatial_shapes,
  sampling_locations: torch.Tensor,
  attention_weights: torch.Tensor,
  num_points_list: list[int],
  method="default",
):
  """
  Args:
      value (Tensor): [bs, value_length, n_head, c]
      value_spatial_shapes (Tensor|List): [n_levels, 2]
      value_level_start_index (Tensor|List): [n_levels]
      sampling_locations (Tensor): [bs, query_length, n_head, n_levels * n_points, 2]
      attention_weights (Tensor): [bs, query_length, n_head, n_levels * n_points]

  Returns:
      output (Tensor): [bs, Length_{query}, C]
  """
  bs, n_head, c, _ = value[0].shape
  _, Len_q, _, _, _ = sampling_locations.shape

  # sampling_offsets [8, 480, 8, 12, 2]
  if method == "default":
    sampling_grids = 2 * sampling_locations - 1

  elif method == "discrete":
    sampling_grids = sampling_locations

  sampling_grids = sampling_grids.permute(0, 2, 1, 3, 4).flatten(0, 1)
  sampling_locations_list = sampling_grids.split(num_points_list, dim=-2)

  sampling_value_list = []
  for level, (h, w) in enumerate(value_spatial_shapes):
    value_l = value[level].reshape(bs * n_head, c, h, w)
    sampling_grid_l: torch.Tensor = sampling_locations_list[level]

    if method == "default":
      sampling_value_l = F.grid_sample(
        value_l, sampling_grid_l, mode="bilinear", padding_mode="zeros", align_corners=False
      )

    elif method == "discrete":
      # n * m, seq, n, 2
      sampling_coord = (sampling_grid_l * torch.tensor([[w, h]], device=value_l.device) + 0.5).to(
        torch.int64
      )

      # FIX ME? for rectangle input
      sampling_coord = sampling_coord.clamp(0, h - 1)
      sampling_coord = sampling_coord.reshape(bs * n_head, Len_q * num_points_list[level], 2)

      s_idx = (
        torch.arange(sampling_coord.shape[0], device=value_l.device)
        .unsqueeze(-1)
        .repeat(1, sampling_coord.shape[1])
      )
      sampling_value_l: torch.Tensor = value_l[
        s_idx, :, sampling_coord[..., 1], sampling_coord[..., 0]
      ]  # n l c

      sampling_value_l = sampling_value_l.permute(0, 2, 1).reshape(
        bs * n_head, c, Len_q, num_points_list[level]
      )

    sampling_value_list.append(sampling_value_l)

  attn_weights = attention_weights.permute(0, 2, 1, 3).reshape(
    bs * n_head, 1, Len_q, sum(num_points_list)
  )
  weighted_sample_locs = torch.concat(sampling_value_list, dim=-1) * attn_weights
  output = weighted_sample_locs.sum(-1).reshape(bs, n_head * c, Len_q)

  return output.permute(0, 2, 1)


def translate_gt(gt, reg_max, reg_scale, up):
  """
  Decodes bounding box ground truth (GT) values into distribution-based GT representations.

  This function maps continuous GT values into discrete distribution bins, which can be used
  for regression tasks in object detection models. It calculates the indices of the closest
  bins to each GT value and assigns interpolation weights to these bins based on their proximity
  to the GT value.

  Args:
      gt (Tensor): Ground truth bounding box values, shape (N, ).
      reg_max (int): Maximum number of discrete bins for the distribution.
      reg_scale (float): Controls the curvature of the Weighting Function.
      up (Tensor): Controls the upper bounds of the Weighting Function.

  Returns:
      Tuple[Tensor, Tensor, Tensor]:
          - indices (Tensor): Index of the left bin closest to each GT value, shape (N, ).
          - weight_right (Tensor): Weight assigned to the right bin, shape (N, ).
          - weight_left (Tensor): Weight assigned to the left bin, shape (N, ).
  """
  gt = gt.reshape(-1)
  function_values = weighting_function(reg_max, up, reg_scale)

  # Find the closest left-side indices for each value
  diffs = function_values.unsqueeze(0) - gt.unsqueeze(1)
  mask = diffs <= 0
  closest_left_indices = torch.sum(mask, dim=1) - 1

  # Calculate the weights for the interpolation
  indices = closest_left_indices.float()

  weight_right = torch.zeros_like(indices)
  weight_left = torch.zeros_like(indices)

  valid_idx_mask = (indices >= 0) & (indices < reg_max)
  valid_indices = indices[valid_idx_mask].long()

  # Obtain distances
  left_values = function_values[valid_indices]
  right_values = function_values[valid_indices + 1]

  left_diffs = torch.abs(gt[valid_idx_mask] - left_values)
  right_diffs = torch.abs(right_values - gt[valid_idx_mask])

  # Valid weights
  weight_right[valid_idx_mask] = left_diffs / (left_diffs + right_diffs)
  weight_left[valid_idx_mask] = 1.0 - weight_right[valid_idx_mask]

  # Invalid weights (out of range)
  invalid_idx_mask_neg = indices < 0
  weight_right[invalid_idx_mask_neg] = 0.0
  weight_left[invalid_idx_mask_neg] = 1.0
  indices[invalid_idx_mask_neg] = 0.0

  invalid_idx_mask_pos = indices >= reg_max
  weight_right[invalid_idx_mask_pos] = 1.0
  weight_left[invalid_idx_mask_pos] = 0.0
  indices[invalid_idx_mask_pos] = reg_max - 0.1

  return indices, weight_right, weight_left


def bbox2distance(points, bbox, reg_max, reg_scale, up, eps=0.1):
  """
  Converts bounding box coordinates to distances from a reference point.

  Args:
      points (Tensor): (n, 4) [x, y, w, h], where (x, y) is the center.
      bbox (Tensor): (n, 4) bounding boxes in "xyxy" format.
      reg_max (float): Maximum bin value.
      reg_scale (float): Controling curvarture of W(n).
      up (Tensor): Controling upper bounds of W(n).
      eps (float): Small value to ensure target < reg_max.

  Returns:
      Tensor: Decoded distances.
  """
  reg_scale = abs(reg_scale)
  left = (points[:, 0] - bbox[:, 0]) / (points[..., 2] / reg_scale + 1e-16) - 0.5 * reg_scale
  top = (points[:, 1] - bbox[:, 1]) / (points[..., 3] / reg_scale + 1e-16) - 0.5 * reg_scale
  right = (bbox[:, 2] - points[:, 0]) / (points[..., 2] / reg_scale + 1e-16) - 0.5 * reg_scale
  bottom = (bbox[:, 3] - points[:, 1]) / (points[..., 3] / reg_scale + 1e-16) - 0.5 * reg_scale
  four_lens = torch.stack([left, top, right, bottom], -1)
  four_lens, weight_right, weight_left = translate_gt(four_lens, reg_max, reg_scale, up)
  if reg_max is not None:
    four_lens = four_lens.clamp(min=0, max=reg_max - eps)
  return four_lens.reshape(-1).detach(), weight_right.detach(), weight_left.detach()


def get_contrastive_denoising_training_group(
  targets,
  num_classes,
  num_queries,
  class_embed,
  num_denoising=100,
  label_noise_ratio=0.5,
  box_noise_scale=1.0,
):
  """cnd"""
  if num_denoising <= 0:
    return None, None, None, None

  num_gts = [len(t["labels"]) for t in targets]
  device = targets[0]["labels"].device

  max_gt_num = max(num_gts)
  if max_gt_num == 0:
    dn_meta = {"dn_positive_idx": None, "dn_num_group": 0, "dn_num_split": [0, num_queries]}
    return None, None, None, dn_meta

  num_group = num_denoising // max_gt_num
  num_group = 1 if num_group == 0 else num_group
  # pad gt to max_num of a batch
  bs = len(num_gts)

  input_query_class = torch.full([bs, max_gt_num], num_classes, dtype=torch.int32, device=device)
  input_query_bbox = torch.zeros([bs, max_gt_num, 4], device=device)
  pad_gt_mask = torch.zeros([bs, max_gt_num], dtype=torch.bool, device=device)

  for i in range(bs):
    num_gt = num_gts[i]
    if num_gt > 0:
      input_query_class[i, :num_gt] = targets[i]["labels"]
      input_query_bbox[i, :num_gt] = targets[i]["boxes"]
      pad_gt_mask[i, :num_gt] = 1
  # each group has positive and negative queries.
  input_query_class = input_query_class.tile([1, 2 * num_group])
  input_query_bbox = input_query_bbox.tile([1, 2 * num_group, 1])
  pad_gt_mask = pad_gt_mask.tile([1, 2 * num_group])
  # positive and negative mask
  negative_gt_mask = torch.zeros([bs, max_gt_num * 2, 1], device=device)
  negative_gt_mask[:, max_gt_num:] = 1
  negative_gt_mask = negative_gt_mask.tile([1, num_group, 1])
  positive_gt_mask = 1 - negative_gt_mask
  # contrastive denoising training positive index
  positive_gt_mask = positive_gt_mask.squeeze(-1) * pad_gt_mask
  dn_positive_idx = torch.nonzero(positive_gt_mask)[:, 1]
  dn_positive_idx = torch.split(dn_positive_idx, [n * num_group for n in num_gts])
  # total denoising queries
  num_denoising = int(max_gt_num * 2 * num_group)

  if label_noise_ratio > 0:
    mask = torch.rand_like(input_query_class, dtype=torch.float) < (label_noise_ratio * 0.5)
    # randomly put a new one here
    new_label = torch.randint_like(mask, 0, num_classes, dtype=input_query_class.dtype)
    input_query_class = torch.where(mask & pad_gt_mask, new_label, input_query_class)

  if box_noise_scale > 0:
    known_bbox = box_cxcywh_to_xyxy(input_query_bbox)
    diff = torch.tile(input_query_bbox[..., 2:] * 0.5, [1, 1, 2]) * box_noise_scale
    rand_sign = torch.randint_like(input_query_bbox, 0, 2) * 2.0 - 1.0
    rand_part = torch.rand_like(input_query_bbox)
    rand_part = (rand_part + 1.0) * negative_gt_mask + rand_part * (1 - negative_gt_mask)
    # shrink_mask = torch.zeros_like(rand_sign)
    # shrink_mask[:, :, :2] = (rand_sign[:, :, :2] == 1)  # rand_sign == 1 → (x1, y1) ↘ →  smaller bbox
    # shrink_mask[:, :, 2:] = (rand_sign[:, :, 2:] == -1)  # rand_sign == -1 →  (x2, y2) ↖ →  smaller bbox
    # mask = rand_part > (upper_bound / (upper_bound+1))
    # # this is to make sure the dn bbox can be reversed to the original bbox by dfine head.
    # rand_sign = torch.where((shrink_mask * (1 - negative_gt_mask) * mask).bool(), \
    #                         rand_sign * upper_bound / (upper_bound+1) / rand_part, rand_sign)
    known_bbox += rand_sign * rand_part * diff
    known_bbox = torch.clip(known_bbox, min=0.0, max=1.0)
    input_query_bbox = box_xyxy_to_cxcywh(known_bbox)
    input_query_bbox[input_query_bbox < 0] *= -1
    input_query_bbox_unact = inverse_sigmoid(input_query_bbox)

  input_query_logits = class_embed(input_query_class)

  tgt_size = num_denoising + num_queries
  attn_mask = torch.full([tgt_size, tgt_size], False, dtype=torch.bool, device=device)
  # match query cannot see the reconstruction
  attn_mask[num_denoising:, :num_denoising] = True

  # reconstruct cannot see each other
  for i in range(num_group):
    if i == 0:
      attn_mask[
        max_gt_num * 2 * i : max_gt_num * 2 * (i + 1), max_gt_num * 2 * (i + 1) : num_denoising
      ] = True
    if i == num_group - 1:
      attn_mask[max_gt_num * 2 * i : max_gt_num * 2 * (i + 1), : max_gt_num * i * 2] = True
    else:
      attn_mask[
        max_gt_num * 2 * i : max_gt_num * 2 * (i + 1), max_gt_num * 2 * (i + 1) : num_denoising
      ] = True
      attn_mask[max_gt_num * 2 * i : max_gt_num * 2 * (i + 1), : max_gt_num * 2 * i] = True

  dn_meta = {
    "dn_positive_idx": dn_positive_idx,
    "dn_num_group": num_group,
    "dn_num_split": [num_denoising, num_queries],
  }

  # print(input_query_class.shape) # torch.Size([4, 196, 256])
  # print(input_query_bbox.shape) # torch.Size([4, 196, 4])
  # print(attn_mask.shape) # torch.Size([496, 496])

  return input_query_logits, input_query_bbox_unact, attn_mask, dn_meta
